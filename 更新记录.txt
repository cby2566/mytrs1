v1.9.6(2023.6.12):
1.cleaner开始使用正则表达式判断，不再需要手动列举特殊字符了，并且添加英文模式
现在的cleaner工作逻辑为：
日文模式下，会自动剔除所有不含日文假名的字符串（只有日文汉字的也会被剔除）
英文模式下，会自动剔除所有不含大小写字母的字符串（换行符\r等不会被认作是字母）
untrsfix同步更新，在没有config文件时，默认是日文模式
v1.9.5.2(2023.6.5):
1.用来替换\r\n的分隔符，现在可以在config里自由调整了
2.导出失败时，将数据输出改为Excel文件
v1.9.5(2023.6.4):
1.更新剔除中文的功能，只在cleaner开始时工作。目的为剔除trs_extractor处理后，残留的部分已汉化文本，推荐平时关闭，因为它也会将日文汉字识别为中文。
v1.9.4(2023.5.11):
1.现在文本过长也不会返回空参数了，但是如果超出引擎限制，可能会返回其他错误代码（每次请求的第一行字符串会无视NPQ的限制进行翻译）
v1.9.3（2023.5.10）：
1.现在会输出完整的报错信息了，方便各位上报bug，也方便我筛查问题所在
v1.9.2（2023.5.7）：
1.修复了会遍历整个enginelist，重复翻译的问题
v1.9.1（2023.5.7）:
1.更新了百度api的请求网址
v1.9（2023.4.24）:
1.config添加一项"cleaner"，用于开关cleaner功能，0关闭，1开启。对于英文翻译，不能使用cleaner功能，用了的话会把英文文本全部删除，以前没注意到。
并将进行判断的特殊字符放在了config的"special"中，可自行删改
2.优化了每次请求中字符量（NPQ）的判断，现在每次请求一定不会超过NPQ了，同时对limits的判断也更为精确，实际消耗会控制在limits-NPQ左右
3.修正了在1.7和1.8更新后发现的bug

v1.8（2023.4.24）:
1.现在在翻译前会先依照文本长短进行排序，优先翻译长文本，因为长文本大概率是剧情文本，这样在翻译引擎字数不足时，这些文本会被优先翻译。
在这种情况下，推荐将翻译效果好的引擎放在enginelist的前面，一种进阶玩法是，手动调整limits的大小，达到用效果比较好的翻译引擎将大部分剧情文本
翻译后，用效果差的翻译引擎去翻译那些不重要的文本。（怎么可能有人能靠手动设置做到这个-_-,用代码比较好的实现这个效果的方法我也没想到，以后再说吧）
（其实这个功能是想用在chatgpt上的，奈何gpt没有免费额度，只能作罢。）

v1.7（2023.4.24）:
1.更换了分隔每行字符的分隔符（从“S/som”换成了“↑☆↓☆”），消除了可能的对英文翻译的影响；
2.现在能够自动记录api通过本工具消耗的总字符量，并且每月初自动重置；
3.删除了config.json中‘engine’项，改为按‘enginelist’中的先后顺序，按顺序使用翻译引擎，当前一个引擎字数达到上限后，自动更换到下一个引擎
若所有引擎的总余量仍不足以翻译整个文件，则会在翻译前提示，并在翻译后将已翻译部分和未翻译部分分别保存在TrsData.bin和untrsed.json中，
配置文件config.json的说明已在“必看！！使用说明.txt”中更新。

v1.6:

1.大概率修正了彩云吞换行符的问题

v1.5：
1.因为百度翻译存在奇怪的bug且找不到原因，直接修改了整个框架，现在的框架比以前的更合理了
2.修正了换行符只考虑到\n而没考虑到\r的问题
3.修正了读取文件失败后闪退的问题（应该停留在报错界面）

4.修正了字词库替换原文后没有替换回来的问题